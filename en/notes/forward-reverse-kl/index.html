<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Forward KLとReverse KL | Yuta Ono</title>
<meta name=keywords content="KL Divergence,ELBO,MLE,Knowledge Distillation"><meta name=description content="KLダイバージェンスは非対称な性質を持つ"><meta name=author content><link rel=canonical href=https://yuta0no.github.io/en/notes/forward-reverse-kl/><link crossorigin=anonymous href=/assets/css/stylesheet.f81382ba3c87e7f5b41013ee56645c6442f333ed4a747c5399c233f90a0a90e1.css integrity="sha256-+BOCujyH5/W0EBPuVmRcZELzM+1KdHxTmcIz+QoKkOE=" rel="preload stylesheet" as=style><link rel=icon href=https://yuta0no.github.io/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yuta0no.github.io/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yuta0no.github.io/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://yuta0no.github.io/favicons/apple-touch-icon.png><link rel=mask-icon href=https://yuta0no.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=ja href=https://yuta0no.github.io/notes/forward-reverse-kl/><link rel=alternate hreflang=en href=https://yuta0no.github.io/en/notes/forward-reverse-kl/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@400;700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>let macros={"\\pdv":"\\frac{\\partial}{\\partial #1}"};document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{macros,delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],trust:!0,throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-N8937T99H2"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-N8937T99H2")}</script><meta property="og:url" content="https://yuta0no.github.io/en/notes/forward-reverse-kl/"><meta property="og:site_name" content="Yuta Ono"><meta property="og:title" content="Forward KLとReverse KL"><meta property="og:description" content="KLダイバージェンスは非対称な性質を持つ"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="notes"><meta property="article:published_time" content="2026-02-21T23:00:00+09:00"><meta property="article:modified_time" content="2026-02-21T23:00:00+09:00"><meta property="article:tag" content="KL Divergence"><meta property="article:tag" content="ELBO"><meta property="article:tag" content="MLE"><meta property="article:tag" content="Knowledge Distillation"><meta property="og:image" content="https://yuta0no.github.io/opengraph.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://yuta0no.github.io/opengraph.png"><meta name=twitter:title content="Forward KLとReverse KL"><meta name=twitter:description content="KLダイバージェンスは非対称な性質を持つ"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Notes","item":"https://yuta0no.github.io/en/notes/"},{"@type":"ListItem","position":2,"name":"Forward KLとReverse KL","item":"https://yuta0no.github.io/en/notes/forward-reverse-kl/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Forward KLとReverse KL","name":"Forward KLとReverse KL","description":"KLダイバージェンスは非対称な性質を持つ","keywords":["KL Divergence","ELBO","MLE","Knowledge Distillation"],"articleBody":" This note is unfinished. I will translate it into English soon. Forward KLはmean-seekingでReverse KLはmode-seeking KLダイバージェンスはふたつの確率分布がどれだけ離れているかを測ることができる「距離」のようなものである（厳密な意味での距離ではないことに注意）．\nたとえば一般の変分推論においては，変分下限（ELBO）を最大化することは真の分布$P$と近似分布$Q$とのKLダイバージェンス $$ D_\\mathrm{KL}(q_\\theta \\| p) = \\int_{Z} q_\\theta(z) \\log \\frac{q_\\theta(z)}{p(z | x)} dz $$ を最小化することと等価であり，ELBO最大化による$Q$の学習は$D_\\mathrm{KL}(q_\\theta \\| p)$の意味で確率分布$q_\\theta(z)$を真の事後分布$p(z|x)$に近づけていると解釈できる．\nKLダイバージェンスは非対称な関数であるため，一般の最適化問題において近似分布$Q$を真の分布$P$にKLダイバージェンスの意味で近づけたい際には $$ \\begin{equation} D_\\mathrm{KL}(P \\| Q) = \\int_{X} P(x) \\log \\frac{P(x)}{Q(x)} dx \\end{equation} $$ と $$ \\begin{equation} D_\\mathrm{KL}(Q \\| P) = \\int_{X} Q(x) \\log \\frac{Q(x)}{P(x)} dx \\end{equation} $$ の2つの選択肢が存在する．\n一般に，式(1)をForward KLと呼び，式(2)をReverse KLと呼ぶ． これら2つのKLダイバージェンスは以下に示すようにそれぞれ異なる性質を持つ．\nForward KL まず，Forward KLは$\\log \\frac{P(x)}{Q(x)}$の$P(x)$についての期待値である． したがって，$P(x)$が比較的大きい値をとり，なおかつ$Q(x) = \\epsilon$（ただし$\\epsilon$は極めて小さな数）となるような区間$[x, x+\\Delta x]$が存在する場合，$P(x) \\log \\frac{P(x)}{Q(x)}$は大きな値をとり，期待値を大きくする．\n逆に，$P(x)=0$であるような区間においては$Q(x)$がどのような値をとっても期待値に影響しない．\nつまり，Forward KLを小さくするためには，$P(x)$の確率密度が非零である領域全体をカバーするように$Q(x)$を動かすことが重要である (zero-avoiding)．\nReverse KL 一方，Reverse KLは$\\log \\frac{Q(x)}{P(x)}$の$Q(x)$についての期待値である．\nForward KLとは異なり，$P(x)$が大きな値をとる領域においても$Q(x)$が小さい値であれば，KLの値に与える影響は小さい． 逆に，$P(x)$が小さい区間で$Q(x)$が大きい場合はKLが大きい値になる． したがって，Reverse KLの最小化においては$P(x)$が小さい区間では$Q(x)$を小さくし，$P(x)$が大きい点に$Q(x)$の確率密度を集中させることが重要である (zero-forcing)． なお，$P(x)$が大きい領域が複数存在する場合でも，そのすべてで$Q(x)$を大きくする必要はないことに注意が必要である．\n最適化時の分布の動き方の例 以下では，ふたつのKLの違いを定性的に確認するために，簡単な設定での最適化問題を例として考える．\n真の分布として $$ P(x) = 0.5 \\times \\mathcal{N}\\left(x; \\begin{bmatrix} -4.0 \\\\ 0 \\end{bmatrix}, I_2\\right) + 0.5 \\times \\mathcal{N}\\left(x; \\begin{bmatrix} 4.0 \\\\ 0 \\end{bmatrix}, I_2\\right) $$ という2つの峰をもつ混合ガウス分布を設定し， $$ Q(x) = \\mathcal{N}(x; \\mu, \\Sigma) $$ で近似する場合を考える．\n特に，$Q$の$\\mu, \\Sigma$を学習可能パラメータとし，Adamを用いた勾配降下法によってForward KLを最小化する場合とReverse KLを最小化する場合の分布$Q$の動き方を比較する． 具体的なハイパーパラメータ等は付録のソースコードを参照されたい．\nなお，この最適化においてはKLダイバージェンスを直接評価するために真の分布$P(x)$からのサンプルを取得したり，$\\log P(x)$を評価したりしているが，現実的な問題設定においては$P(x)$が未知であるためそのような操作が難しいことに注意が必要である（変分推論ではこの問題を回避するためにKLを直接最小化する代わりにELBOを最大化しているとも言える）．\nForward KL 以下の動画は，反復法により$Q$がどのように変化するかを表している． 青色の等高線は$P$を表し，赤色のバツ印および楕円はそれぞれ$Q$の平均と$2\\sigma$区間を表す．\nForward KLの最小化では，$P$の2つの峰の中心に$Q$の平均が近づいていき，多峰分布全体を覆うように共分散を大きくしていく (mean-seeking)． これは，Forward KLは峰の取りこぼしを嫌うためである．\nReverse KL Reverse KLの最小化では$P$の片方の峰だけに$Q$が近づいていき，他方の峰は無視されている (mode-seeking)． 峰の取りこぼしはReverse KLの値に影響しない一方で，$P(x)$が小さい領域で$Q(x)$が大きくなることを嫌うためである．\nしたがって，Reverse KLによって$Q$を最適化する場合には，分散を過小評価してしまう（over-confidentになる）傾向がある．\nそれぞれのKLの用途 ここまでForward KLとReverse KLが異なる性質を持つことを確認してきた． 以下ではそれらのKLが実際の最適化や機械学習においてどのように利用されているかを考える．\nまずReverse KLについては，先述したように変分ベイズにおける最小化対象として自然と導出される． Reverse KLの計算においては近似分布$Q$からのサンプルを用いて期待値が近似できるため，計算が扱いやすいという特長を持っている． これは最適化において求めたい分布$P$からのサンプルを取得することは難しい一方で，近似分布$Q$については具体的な形がわかっているためである．\n最尤推定によるモデル学習はForward KLの最小化と同一視できる．\n最尤推定は学習対象のデータ分布$p_\\mathrm{data}(x)$に対して尤度関数$\\log q(x | \\theta)$が最大となるような$\\theta$を求める方法である． すなわち $$ \\mathbb{E}_{p_\\mathrm{data}(x)} \\left[ \\log q(x | \\theta) \\right] $$ を最大化することで，観測データをうまく説明するモデルパラメータ$\\theta$を推定する．\nここで $$ \\mathbb{E}_{p_\\mathrm{data}(x)} \\left[ \\log q(x | \\theta) \\right] = \\mathbb{E}_{p_\\mathrm{data}(x)} \\left[ \\log p_\\mathrm{data}(x) \\right] - D_\\mathrm{KL}(p_\\mathrm{data}(x) \\| q(x | \\theta)) $$ であり，右辺第1項は$p_\\mathrm{data}(x)$固定のもとでは定数であることから，左辺を最大化することは右辺第2項のForward KLを最小化することと等価であることがわかる．\n他には知識蒸留においてもForward KLが用いられることが多い1． 具体的には，教師モデルを$q_t(x)$，生徒モデルを$q_s(x)$として $$ D_\\mathrm{KL}(q_t \\| q_s) $$ を損失の一部として利用する． Forward KLの性質を考慮すると，分類モデルに対してこのような損失を利用して知識蒸留した場合には，教師モデルが少しでも確率質量を割り当てたクラスについては生徒モデルの予測確率も非零となるよう訓練されることが想定される．\nなお，LLMの知識蒸留の文脈においては，Forward KLが分布の峰に着目し，Reverse KLが分布の裾に着目する傾向をもつという発見に基づき，Forward KLだけでなくReverse KLも適応的に利用する方法が提案されているようである2．\n付録 以下に本記事に掲載した動画を作成するためのpythonスクリプトを示す．\nRead Also “KL Divergence: Forward vs Reverse?,” Agustinus Kristiadi Hinton+., “Distilling the Knowledge in a Neural Network,” NIPS 2014 Workshop ↩︎\nWu+, “Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models,” COLING 2025 ↩︎\n","wordCount":"2944","inLanguage":"en","image":"https://yuta0no.github.io/opengraph.png","datePublished":"2026-02-21T23:00:00+09:00","dateModified":"2026-02-21T23:00:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://yuta0no.github.io/en/notes/forward-reverse-kl/"},"publisher":{"@type":"Organization","name":"Yuta Ono","logo":{"@type":"ImageObject","url":"https://yuta0no.github.io/favicons/favicon.ico"}}}</script><link rel=stylesheet href=https://yuta0no.github.io/sass/colored_note.min.347c43f01ea26c5f3c825cab42216e8a5a0ba7573b829c5d238fb06feee1e06b.css></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://yuta0no.github.io/en/ accesskey=h title="Yuta Ono (Alt + H)">Yuta Ono</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://yuta0no.github.io/ title=日本語 aria-label=日本語>Ja</a></li></ul></div></div><ul id=menu><li><a href=https://yuta0no.github.io/en/profile/ title=Profile><span>Profile</span></a></li><li><a href=https://yuta0no.github.io/en/research/ title=Research><span>Research</span></a></li><li><a href=https://yuta0no.github.io/en/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://yuta0no.github.io/en/notes/ title=Notes><span>Notes</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://yuta0no.github.io/en/>Home</a>&nbsp;»&nbsp;<a href=https://yuta0no.github.io/en/notes/>Notes</a></div><h1 class="post-title entry-hint-parent">Forward KLとReverse KL</h1><div class=post-meta><span title='2026-02-21 23:00:00 +0900 +0900'>February 21, 2026</span>&nbsp;·&nbsp;<span>6 min</span>&nbsp;|&nbsp;<span>Translations:</span><ul class=i18n_list><li><a href=https://yuta0no.github.io/notes/forward-reverse-kl/>Ja</a></li></ul></div></header><div class=post-content><aside><div class='note cn-b--red cn-bg--red'><p>This note is unfinished. I will translate it into English soon.</p></div></aside><aside><div class='note cn-b--blue cn-bg--blue'><p>Forward KLはmean-seekingでReverse KLはmode-seeking</p></div></aside><p>KLダイバージェンスはふたつの確率分布がどれだけ離れているかを測ることができる「距離」のようなものである（厳密な意味での距離ではないことに注意）．</p><p>たとえば一般の<a href=/en/notes/two-ways-of-elbo-derivation/>変分推論</a>においては，変分下限（ELBO）を最大化することは真の分布$P$と近似分布$Q$とのKLダイバージェンス
$$
D_\mathrm{KL}(q_\theta \| p) = \int_{Z} q_\theta(z) \log \frac{q_\theta(z)}{p(z | x)} dz
$$
を最小化することと等価であり，ELBO最大化による$Q$の学習は$D_\mathrm{KL}(q_\theta \| p)$の意味で確率分布$q_\theta(z)$を真の事後分布$p(z|x)$に近づけていると解釈できる．</p><p>KLダイバージェンスは非対称な関数であるため，一般の最適化問題において近似分布$Q$を真の分布$P$にKLダイバージェンスの意味で近づけたい際には
$$
\begin{equation}
D_\mathrm{KL}(P \| Q) = \int_{X} P(x) \log \frac{P(x)}{Q(x)} dx
\end{equation}
$$
と
$$
\begin{equation}
D_\mathrm{KL}(Q \| P) = \int_{X} Q(x) \log \frac{Q(x)}{P(x)} dx
\end{equation}
$$
の2つの選択肢が存在する．</p><p>一般に，式(1)をForward KLと呼び，式(2)をReverse KLと呼ぶ．
これら2つのKLダイバージェンスは以下に示すようにそれぞれ異なる性質を持つ．</p><h2 id=forward-kl>Forward KL<a hidden class=anchor aria-hidden=true href=#forward-kl>#</a></h2><p>まず，Forward KLは$\log \frac{P(x)}{Q(x)}$の$P(x)$についての期待値である．
したがって，$P(x)$が比較的大きい値をとり，なおかつ$Q(x) = \epsilon$（ただし$\epsilon$は極めて小さな数）となるような区間$[x, x+\Delta x]$が存在する場合，$P(x) \log \frac{P(x)}{Q(x)}$は大きな値をとり，期待値を大きくする．</p><p>逆に，$P(x)=0$であるような区間においては$Q(x)$がどのような値をとっても期待値に影響しない．</p><p>つまり，Forward KLを小さくするためには，$P(x)$の確率密度が非零である領域全体をカバーするように$Q(x)$を動かすことが重要である (zero-avoiding)．</p><h2 id=reverse-kl>Reverse KL<a hidden class=anchor aria-hidden=true href=#reverse-kl>#</a></h2><p>一方，Reverse KLは$\log \frac{Q(x)}{P(x)}$の$Q(x)$についての期待値である．</p><p>Forward KLとは異なり，$P(x)$が大きな値をとる領域においても$Q(x)$が小さい値であれば，KLの値に与える影響は小さい．
逆に，$P(x)$が小さい区間で$Q(x)$が大きい場合はKLが大きい値になる．
したがって，Reverse KLの最小化においては$P(x)$が小さい区間では$Q(x)$を小さくし，$P(x)$が大きい点に$Q(x)$の確率密度を集中させることが重要である (zero-forcing)．
なお，$P(x)$が大きい領域が複数存在する場合でも，そのすべてで$Q(x)$を大きくする必要はないことに注意が必要である．</p><h2 id=最適化時の分布の動き方の例>最適化時の分布の動き方の例<a hidden class=anchor aria-hidden=true href=#最適化時の分布の動き方の例>#</a></h2><p>以下では，ふたつのKLの違いを定性的に確認するために，簡単な設定での最適化問題を例として考える．</p><p>真の分布として
$$
P(x) = 0.5 \times \mathcal{N}\left(x; \begin{bmatrix} -4.0 \\ 0 \end{bmatrix}, I_2\right) + 0.5 \times \mathcal{N}\left(x; \begin{bmatrix} 4.0 \\ 0 \end{bmatrix}, I_2\right)
$$
という2つの峰をもつ混合ガウス分布を設定し，
$$
Q(x) = \mathcal{N}(x; \mu, \Sigma)
$$
で近似する場合を考える．</p><p>特に，$Q$の$\mu, \Sigma$を学習可能パラメータとし，Adamを用いた勾配降下法によってForward KLを最小化する場合とReverse KLを最小化する場合の分布$Q$の動き方を比較する．
具体的なハイパーパラメータ等は<a href=/en/notes/forward-reverse-kl/#appendix>付録</a>のソースコードを参照されたい．</p><p>なお，この最適化においてはKLダイバージェンスを直接評価するために真の分布$P(x)$からのサンプルを取得したり，$\log P(x)$を評価したりしているが，現実的な問題設定においては$P(x)$が未知であるためそのような操作が難しいことに注意が必要である（変分推論ではこの問題を回避するためにKLを直接最小化する代わりにELBOを最大化しているとも言える）．</p><h3 id=forward-kl-1>Forward KL<a hidden class=anchor aria-hidden=true href=#forward-kl-1>#</a></h3><p>以下の動画は，反復法により$Q$がどのように変化するかを表している．
青色の等高線は$P$を表し，赤色のバツ印および楕円はそれぞれ$Q$の平均と$2\sigma$区間を表す．</p><p>Forward KLの最小化では，$P$の2つの峰の中心に$Q$の平均が近づいていき，多峰分布全体を覆うように共分散を大きくしていく (mean-seeking)．
これは，Forward KLは峰の取りこぼしを嫌うためである．</p><video controls width=100%>
<source src=/video/forward.mp4 type=video/mp4><p></p></video><h3 id=reverse-kl-1>Reverse KL<a hidden class=anchor aria-hidden=true href=#reverse-kl-1>#</a></h3><p>Reverse KLの最小化では$P$の片方の峰だけに$Q$が近づいていき，他方の峰は無視されている (mode-seeking)．
峰の取りこぼしはReverse KLの値に影響しない一方で，$P(x)$が小さい領域で$Q(x)$が大きくなることを嫌うためである．</p><p>したがって，Reverse KLによって$Q$を最適化する場合には，分散を過小評価してしまう（over-confidentになる）傾向がある．</p><video controls width=100%>
<source src=/video/reverse.mp4 type=video/mp4><p></p></video><h2 id=それぞれのklの用途>それぞれのKLの用途<a hidden class=anchor aria-hidden=true href=#それぞれのklの用途>#</a></h2><p>ここまでForward KLとReverse KLが異なる性質を持つことを確認してきた．
以下ではそれらのKLが実際の最適化や機械学習においてどのように利用されているかを考える．</p><p>まずReverse KLについては，先述したように変分ベイズにおける最小化対象として自然と導出される．
Reverse KLの計算においては近似分布$Q$からのサンプルを用いて期待値が近似できるため，計算が扱いやすいという特長を持っている．
これは最適化において求めたい分布$P$からのサンプルを取得することは難しい一方で，近似分布$Q$については具体的な形がわかっているためである．</p><p>最尤推定によるモデル学習はForward KLの最小化と同一視できる．</p><p>最尤推定は学習対象のデータ分布$p_\mathrm{data}(x)$に対して尤度関数$\log q(x | \theta)$が最大となるような$\theta$を求める方法である．
すなわち
$$
\mathbb{E}_{p_\mathrm{data}(x)} \left[ \log q(x | \theta) \right]
$$
を最大化することで，観測データをうまく説明するモデルパラメータ$\theta$を推定する．</p><p>ここで
$$
\mathbb{E}_{p_\mathrm{data}(x)} \left[ \log q(x | \theta) \right] = \mathbb{E}_{p_\mathrm{data}(x)} \left[ \log p_\mathrm{data}(x) \right] - D_\mathrm{KL}(p_\mathrm{data}(x) \| q(x | \theta))
$$
であり，右辺第1項は$p_\mathrm{data}(x)$固定のもとでは定数であることから，左辺を最大化することは右辺第2項のForward KLを最小化することと等価であることがわかる．</p><p>他には知識蒸留においてもForward KLが用いられることが多い<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>．
具体的には，教師モデルを$q_t(x)$，生徒モデルを$q_s(x)$として
$$
D_\mathrm{KL}(q_t \| q_s)
$$
を損失の一部として利用する．
Forward KLの性質を考慮すると，分類モデルに対してこのような損失を利用して知識蒸留した場合には，教師モデルが少しでも確率質量を割り当てたクラスについては生徒モデルの予測確率も非零となるよう訓練されることが想定される．</p><p>なお，LLMの知識蒸留の文脈においては，Forward KLが分布の峰に着目し，Reverse KLが分布の裾に着目する傾向をもつという発見に基づき，Forward KLだけでなくReverse KLも適応的に利用する方法が提案されているようである<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>．</p><h2 id=付録>付録<a hidden class=anchor aria-hidden=true href=#付録>#</a></h2><p>以下に本記事に掲載した動画を作成するためのpythonスクリプトを示す．</p><script src=https://gist.github.com/Yuta0no/6be41d50d602e5cfcf71272e21c2bd9d.js></script><h2 id=read-also>Read Also<a hidden class=anchor aria-hidden=true href=#read-also>#</a></h2><ul><li><a href=https://agustinus.kristia.de/blog/forward-reverse-kl/>&ldquo;KL Divergence: Forward vs Reverse?,&rdquo; Agustinus Kristiadi</a></li></ul><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://arxiv.org/abs/1503.02531>Hinton+., &ldquo;Distilling the Knowledge in a Neural Network,&rdquo; NIPS 2014 Workshop</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://arxiv.org/abs/2404.02657>Wu+, &ldquo;Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models,&rdquo; COLING 2025</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://yuta0no.github.io/en/tags/kl-divergence/>KL Divergence</a></li><li><a href=https://yuta0no.github.io/en/tags/elbo/>ELBO</a></li><li><a href=https://yuta0no.github.io/en/tags/mle/>MLE</a></li><li><a href=https://yuta0no.github.io/en/tags/knowledge-distillation/>Knowledge Distillation</a></li></ul><nav class=paginav><a class=next href=https://yuta0no.github.io/en/notes/two-ways-of-elbo-derivation/><span class=title>Next »</span><br><span>変分ベイズにおける2通りのELBO導出について</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://yuta0no.github.io/en/>Yuta Ono</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>